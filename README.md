# 🧠 Advancing Medical LLMs: A Multi-Metric Evaluation Framework

This repository presents a unified, retrieval-grounded evaluation framework for benchmarking medical large language models (LLMs) across factual and human-aligned dimensions. The pipeline is built in a single Jupyter notebook and evaluates real-world patient queries using domain-relevant biomedical evidence.

---

## 🔍 Overview

This project introduces a comprehensive evaluation strategy to assess medical LLM performance using real patient queries. The framework scores responses on six critical metrics:

- ✅ **Correctness**
- 🚫 **Hallucination Resistance**
- 📋 **Completeness**
- 🔁 **Faithfulness**
- 📚 **Groundedness**
- 💬 **Empathy**

### 🧪 Evaluated Models
- **ChatGPT** (GPT-3.5-turbo, OpenAI)
- **Claude** (Anthropic)
- **DeepSeek V3**

---

## 🎯 Objectives

- Benchmark commercial LLMs on real-world clinical questions.
- Evaluate models using a holistic set of six complementary metrics.
- Use PubMed/EuropePMC literature as factual grounding references.
- Identify model behavior trade-offs, especially between empathy and accuracy.
- Provide interpretable results through visual summaries and structured outputs.

---

## 📦 Prerequisites

Before running the pipeline, make sure you have:

- Python 3.8+
- Personal email ID registered with:
  - OpenAI
  - Anthropic Claude
  - DeepSeek
- Active API keys for:
  - OpenAI (for GPT-3.5-turbo or GPT-4 gold synthesis)
  - Anthropic Claude (Claude 3 Opus)
  - DeepSeek V3
- Basic familiarity with:
  - Jupyter Notebooks or Python scripting
  - LangChain and OpenAI API
  - PubMed/EuropePMC for biomedical document retrieval

- Installed Python packages:
  - `openai`, `langchain`, `faiss-cpu`, `pandas`, `matplotlib`, `seaborn`, `biopython`

---

## ⚙️ How It Works
> 💡 **Note**: Each LLM requires individual API keys tied to your personal email/account. Keys must be manually configured in the code or via environment variables.

All logic is implemented in a single Jupyter notebook:

### 🧮 Main Pipeline

📄 **File**: `MedicalLLM_Evaluation_Pipeline.ipynb`

- Accepts real-world queries from ChatDoctor (32 queries used in this pilot study).
- Retrieves biomedical documents from PubMed and EuropePMC.
- Generates LLM responses via LangChain (zero-shot, deterministic settings).
- Embeds and ranks evidence documents using FAISS and OpenAI embeddings.
- Synthesizes a pseudo-gold document using GPT-4.
- Scores model responses using prompt-based evaluation across 6 metrics.
- Outputs scores into a structured CSV for analysis.

### 📊 Result Visualization

📄 **File**: `Results_Visualizer.ipynb`

- Loads `capstone_results.csv` generated by the pipeline.
- Generates:
  - Bar plots of average metric scores
  - Radar chart of overall model profiles
  - Box plots and dot plots for score distribution
  - Faithfulness vs. empathy trade-off chart
  - Heatmaps and ranking tables

---

## 🗂️ Project Structure

```bash
LLMs-Healthcare-Evaluation/
│
├── MedicalLLM_Evaluation_Pipeline.ipynb      # 🔧 End-to-end pipeline
├── Results_Visualizer.ipynb                  # 📊 Metrics visualization
├── capstone_results.csv                      # 📁 Final metric scores
├── README.md                                 # 📄 Project overview and instructions
│
├── Trials/                                   # 🧪 Prior metric experiments
│   ├── Accuracy/                             # 🔍 Accuracy trials
│   ├── Empathy/                              # 💬 Empathy evaluation tests
│   └── data.csv                              # 🧾 Input
│
└── .git/                                     # 🌱 Git version control

```
---
## 🧪 Results

We evaluated ChatGPT, Claude, and DeepSeek across six key metrics using our multi-metric evaluation framework.

### 📊 Metric-Wise Model Ranking (Higher is Better)

| **Metric**       | **1st**                  | **2nd**                   | **3rd**                   |
|------------------|--------------------------|----------------------------|----------------------------|
| Correctness      | DeepSeek (8.04)          | ChatGPT (7.89)             | Claude (7.86)              |
| Hallucination    | DeepSeek (8.43)          | ChatGPT (8.39)             | Claude (8.21)              |
| Completeness     | DeepSeek (8.07)          | ChatGPT (7.89)             | Claude (7.54)              |
| Faithfulness     | DeepSeek (7.29)          | Claude (7.25)              | ChatGPT (7.00)             |
| Groundedness     | Claude (6.32)            | DeepSeek (6.32)            | ChatGPT (6.07)             |
| Empathy          | DeepSeek (8.82)          | ChatGPT (8.32)             | Claude (7.32)              |

> 🔍 **Key Insights**:  
> - **DeepSeek** outperformed on factual metrics like correctness, hallucination, and completeness.  
> - **Claude** led on groundedness and was consistent across metrics.  
> - **ChatGPT** demonstrated strong empathy and competitive completeness.

---

## 👥 Team Members

- Archana Adhi  
- Havanitha Macha  
- Shravan Busireddy

