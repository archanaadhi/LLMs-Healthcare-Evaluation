# Comparative Evaluation of Large Language Models for Healthcare Applications
## ðŸ“Œ Project Overview
This project aims to **evaluate the performance of Large Language Models (LLMs) in healthcare applications**. With the increasing adoption of AI in medicine, it is essential to ensure that LLM-generated responses are **accurate, complete, empathetic, and unbiased**.  

The project automates the evaluation process using well-defined metrics and structured methodologies. It assesses multiple LLMs (**LLaMA, DeepSeek, BERT**) on medical datasets and utilizes an **evaluation LLM (ChatGPT or Med-PaLM)** to analyze their responses.

---

## ðŸŽ¯ Objectives
- **Data Collection & Processing**: Gather diverse **medical question-answer datasets**.
- **Model Response Generation**: Use **LLaMA, DeepSeek, and BERT** to generate responses and other such LLMs depending on dataset requirements.
- **Defining Evaluation Metrics**: Establish correctness, completeness, coherence, empathy, and bias detection methods.
- **Automated Response Assessment**: Train an evaluation model to compare and analyze responses.
- **Optimization via RAG**: Enhance responses by integrating **retrieved medical literature**.
- **Deployment & Continuous Monitoring**: Build an API-based evaluation system.

---

## ðŸ”¹ Prerequisites
Ensure you have the following installed:
- **Python 3.8+**
- **pip** (Python package manager)
- **Git**
- **Hugging Face `transformers` library**
- **PyTorch or TensorFlow**
- **NLTK & SciSpacy** (for NLP preprocessing)

---

## Team Members
* Archana Adhi
* Havanitha Macha
* Shravan Busireddy
