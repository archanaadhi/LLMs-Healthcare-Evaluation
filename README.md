# ğŸ§  Advancing Medical LLMs: A Multi-Metric Evaluation Framework

This repository presents a unified, retrieval-grounded evaluation framework for benchmarking medical large language models (LLMs) across factual and human-aligned dimensions. The pipeline is built in a single Jupyter notebook and evaluates real-world patient queries using domain-relevant biomedical evidence.

---

## ğŸ” Overview

This project introduces a comprehensive evaluation strategy to assess medical LLM performance using real patient queries. The framework scores responses on six critical metrics:

- âœ… **Correctness**
- ğŸš« **Hallucination Resistance**
- ğŸ“‹ **Completeness**
- ğŸ” **Faithfulness**
- ğŸ“š **Groundedness**
- ğŸ’¬ **Empathy**

### ğŸ§ª Evaluated Models
- **ChatGPT** (GPT-3.5-turbo, OpenAI)
- **Claude** (Anthropic)
- **DeepSeek V3**

---

## ğŸ¯ Objectives

- Benchmark commercial LLMs on real-world clinical questions.
- Evaluate models using a holistic set of six complementary metrics.
- Use PubMed/EuropePMC literature as factual grounding references.
- Identify model behavior trade-offs, especially between empathy and accuracy.
- Provide interpretable results through visual summaries and structured outputs.

---

## ğŸ“¦ Prerequisites

Before running the pipeline, make sure you have:

- Python 3.8+
- Personal email ID registered with:
  - OpenAI
  - Anthropic Claude
  - DeepSeek
- Active API keys for:
  - OpenAI (for GPT-3.5-turbo or GPT-4 gold synthesis)
  - Anthropic Claude (Claude 3 Opus)
  - DeepSeek V3
- Basic familiarity with:
  - Jupyter Notebooks or Python scripting
  - LangChain and OpenAI API
  - PubMed/EuropePMC for biomedical document retrieval

- Installed Python packages:
  - `openai`, `langchain`, `faiss-cpu`, `pandas`, `matplotlib`, `seaborn`, `biopython`

---

## âš™ï¸ How It Works
> ğŸ’¡ **Note**: Each LLM requires individual API keys tied to your personal email/account. Keys must be manually configured in the code or via environment variables.

All logic is implemented in a single Jupyter notebook:

### ğŸ§® Main Pipeline

ğŸ“„ **File**: `MedicalLLM_Evaluation_Pipeline.ipynb`

- Accepts real-world queries from ChatDoctor (32 queries used in this pilot study).
- Retrieves biomedical documents from PubMed and EuropePMC.
- Generates LLM responses via LangChain (zero-shot, deterministic settings).
- Embeds and ranks evidence documents using FAISS and OpenAI embeddings.
- Synthesizes a pseudo-gold document using GPT-4.
- Scores model responses using prompt-based evaluation across 6 metrics.
- Outputs scores into a structured CSV for analysis.

### ğŸ“Š Result Visualization

ğŸ“„ **File**: `Results_Visualizer.ipynb`

- Loads `capstone_results.csv` generated by the pipeline.
- Generates:
  - Bar plots of average metric scores
  - Radar chart of overall model profiles
  - Box plots and dot plots for score distribution
  - Faithfulness vs. empathy trade-off chart
  - Heatmaps and ranking tables

---

## ğŸ—‚ï¸ Project Structure

```bash
LLMs-Healthcare-Evaluation/
â”‚
â”œâ”€â”€ MedicalLLM_Evaluation_Pipeline.ipynb      # ğŸ”§ End-to-end pipeline
â”œâ”€â”€ Results_Visualizer.ipynb                  # ğŸ“Š Metrics visualization
â”œâ”€â”€ capstone_results.csv                      # ğŸ“ Final metric scores
â”œâ”€â”€ README.md                                 # ğŸ“„ Project overview and instructions
â”‚
â”œâ”€â”€ Trials/                                   # ğŸ§ª Prior metric experiments
â”‚   â”œâ”€â”€ Accuracy/                             # ğŸ” Accuracy trials
â”‚   â”œâ”€â”€ Empathy/                              # ğŸ’¬ Empathy evaluation tests
â”‚   â””â”€â”€ data.csv                              # ğŸ§¾ Input
â”‚
â””â”€â”€ .git/                                     # ğŸŒ± Git version control

```
---
## ğŸ§ª Results

We evaluated ChatGPT, Claude, and DeepSeek across six key metrics using our multi-metric evaluation framework.

### ğŸ“Š Metric-Wise Model Ranking (Higher is Better)

| **Metric**       | **1st**                  | **2nd**                   | **3rd**                   |
|------------------|--------------------------|----------------------------|----------------------------|
| Correctness      | DeepSeek (8.04)          | ChatGPT (7.89)             | Claude (7.86)              |
| Hallucination    | DeepSeek (8.43)          | ChatGPT (8.39)             | Claude (8.21)              |
| Completeness     | DeepSeek (8.07)          | ChatGPT (7.89)             | Claude (7.54)              |
| Faithfulness     | DeepSeek (7.29)          | Claude (7.25)              | ChatGPT (7.00)             |
| Groundedness     | Claude (6.32)            | DeepSeek (6.32)            | ChatGPT (6.07)             |
| Empathy          | DeepSeek (8.82)          | ChatGPT (8.32)             | Claude (7.32)              |

> ğŸ” **Key Insights**:  
> - **DeepSeek** outperformed on factual metrics like correctness, hallucination, and completeness.  
> - **Claude** led on groundedness and was consistent across metrics.  
> - **ChatGPT** demonstrated strong empathy and competitive completeness.

---

## ğŸ‘¥ Team Members

- Archana Adhi  
- Havanitha Macha  
- Shravan Busireddy

