{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11165777,"sourceType":"datasetVersion","datasetId":6967857}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\n\n# Load the dataset\nfile_path = \"/kaggle/input/data-chatgpt/data_chatgpt.csv\"  # Update path if needed\ndf = pd.read_csv(file_path, encoding='ISO-8859-1')  # Fixes UnicodeDecodeError\n\n# Load the empathy classifier model\nclassifier = pipeline(\"text-classification\", model=\"mrm8488/t5-base-finetuned-emotion\", return_all_scores=True)\n\n# Function to get empathy score (using 'sadness', 'fear', etc.)\ndef get_empathy_score(text):\n    try:\n        result = classifier(text[:512])[0]  # Truncate to avoid max length error\n        empathy_emotions = [\"sadness\", \"fear\", \"joy\", \"love\"]  # Emotions we may care about\n        scores = {item['label']: item['score'] for item in result if item['label'] in empathy_emotions}\n        # Return highest scoring empathy-related emotion and its score\n        if scores:\n            top_emotion = max(scores, key=scores.get)\n            return pd.Series([top_emotion, scores[top_emotion]])\n        else:\n            return pd.Series([\"none\", 0.0])\n    except:\n        return pd.Series([\"error\", 0.0])\n\n# Apply to both doctor (output) and LLaMA responses\ndf[['doctor_emotion', 'doctor_empathy_score']] = df['output'].apply(get_empathy_score)\ndf[['llama_emotion', 'llama_empathy_score']] = df['llama response '].apply(get_empathy_score)\n\n# Save result\ndf.to_csv(\"/kaggle/working/empathy_comparison_results.csv\", index=False)\n\nprint(\"✅ Empathy analysis complete. Results saved as empathy_comparison_results.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T20:51:54.281174Z","iopub.execute_input":"2025-03-25T20:51:54.281519Z","iopub.status.idle":"2025-03-25T20:52:39.955802Z","shell.execute_reply.started":"2025-03-25T20:51:54.281496Z","shell.execute_reply":"2025-03-25T20:52:39.954571Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03f96e4a0c64403598f43a70ea0fda44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5289f928c9fc476ba353d48d94d437a2"}},"metadata":{}},{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at mrm8488/t5-base-finetuned-emotion and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba289501c8914b34baf5a41971810713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7df17363c5b847cb957103455e49c6ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab44e7dc4f814a37a15458a7a7826525"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nDevice set to use cpu\n/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Empathy analysis complete. Results saved as empathy_comparison_results.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\n\n# Load the dataset\nfile_path = \"/kaggle/input/data-chatgpt/data_chatgpt.csv\"  # Use the correct path\ndf = pd.read_csv(file_path, encoding='ISO-8859-1')  # Use correct encoding to avoid decode errors\n\n# Load a better emotion detection model\nclassifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n\n# Function to predict empathy (label and score) with input truncation\ndef is_empathetic(text):\n    text = str(text)[:300]  # Truncate long medical responses to 300 characters\n    result = classifier(text)[0]\n    return result['label'], result['score']\n\n# Apply to both doctor and ChatGPT responses\ndf[\"doctor_empathy_label\"], df[\"doctor_empathy_score\"] = zip(*df[\"output\"].apply(is_empathetic))\ndf[\"chatgpt_empathy_label\"], df[\"chatgpt_empathy_score\"] = zip(*df[\"llama response \"].apply(is_empathetic))\n\n# Save results\ndf.to_csv(\"/kaggle/working/empathy_comparison_results.csv\", index=False)\nprint(\"✅ Empathy comparison complete and saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T21:11:06.246495Z","iopub.execute_input":"2025-03-25T21:11:06.246930Z","iopub.status.idle":"2025-03-25T21:11:10.959041Z","shell.execute_reply.started":"2025-03-25T21:11:06.246896Z","shell.execute_reply":"2025-03-25T21:11:10.957819Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Empathy comparison complete and saved!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\n\n# Load the dataset\nfile_path = \"/kaggle/input/data-chatgpt/data_chatgpt.csv\"  # Use the correct path\ndf = pd.read_csv(file_path, encoding='ISO-8859-1')  # Handle encoding errors\n\n# Load the emotion detection model\nclassifier = pipeline(\"text-classification\", model=\"nateraw/bert-base-uncased-emotion\")\n\n# Function to predict empathy (label and score) with input truncation\ndef is_empathetic(text):\n    text = str(text)[:300]  # Truncate long medical responses\n    result = classifier(text)[0]\n    return result['label'], result['score']\n\n# Apply model to both doctor and ChatGPT responses\ndf[\"doctor_empathy_label\"], df[\"doctor_empathy_score\"] = zip(*df[\"output\"].apply(is_empathetic))\ndf[\"chatgpt_empathy_label\"], df[\"chatgpt_empathy_score\"] = zip(*df[\"llama response \"].apply(is_empathetic))\n\n# Save results\ndf.to_csv(\"/kaggle/working/empathy_comparison_results.csv\", index=False)\nprint(\"✅ Empathy comparison complete and saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T21:25:23.944772Z","iopub.execute_input":"2025-03-25T21:25:23.945937Z","iopub.status.idle":"2025-03-25T21:25:33.644766Z","shell.execute_reply.started":"2025-03-25T21:25:23.945873Z","shell.execute_reply":"2025-03-25T21:25:33.643353Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"✅ Empathy comparison complete and saved!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"We tested three emotion detection models: nateraw/bert-base-uncased-emotion, bhadresh-savani/distilbert-base-uncased-emotion, and mrm8488/t5-base-finetuned-emotion. All models classify emotions such as joy, sadness, anger, and fear, which we used to approximate empathy in patient-doctor and ChatGPT responses. The T5 model, while promising, generated inconsistent outputs and often returned incomplete or irrelevant labels, making it unsuitable for our evaluation. The nateraw model also underperformed by frequently labeling sensitive medical queries as \"joy,\" which didn't align with the context. Ultimately, the bhadresh-savani model produced more realistic and relevant labels—like sadness and fear—making it the most reliable empathy classifier for our project.\n","metadata":{}}]}