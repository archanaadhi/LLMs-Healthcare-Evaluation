## üß† Empathy Evaluation: What Worked & What Didn‚Äôt

### ‚úÖ Techniques That Worked

**1. Few-Shot Prompting with GPT-3.5 (OpenAI API)**  
- **What we did:** Prompted GPT-3.5 with labeled examples to rate empathy (1‚Äì10).  
- **Why it worked:** Clean, consistent prompts led to reliable scores.  
- **Best for:** Small to medium-scale evaluations with well-tuned prompts.  
- ‚ö†Ô∏è **Limitation:** Ran into API quota limits during extended runs.

---

### ‚ö†Ô∏è Techniques That Didn't Work Well

**1. `nateraw/bert-base-uncased-emotion`**  
- Labeled serious responses with irrelevant emotions like "joy".  
- ‚ùå Not trained on medical data ‚Äî poor domain relevance.

**2. `mrm8488/t5-base-finetuned-emotion`**  
- Outputs were noisy and difficult to parse.  
- ‚ùå No clear confidence or numeric empathy scoring.

**3. `distilbert-base-uncased-emotion` (Transformer-based)**  
- Misclassified serious/neutral responses as "joy".  
- ‚ùå Could not capture supportive or empathetic tone.  
- ‚úîÔ∏è Works better for social media or general emotional tone.

---
